{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from  torch.utils.data import Dataset,DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(train_or_test,num=None):\n",
    "    with open(os.path.join(\".\",\"data\",train_or_test + \".txt\"),encoding=\"utf-8\") as f:\n",
    "        all_data = f.read().split(\"\\n\")\n",
    "\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for data in all_data:\n",
    "        if data:\n",
    "            t,l = data.split(\"\\t\")\n",
    "            texts.append(t)\n",
    "            labels.append(l)\n",
    "    if num == None:\n",
    "        return texts,labels\n",
    "    else:\n",
    "        return texts[:num],labels[:num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def built_corpus(train_texts,embedding_num):\n",
    "    word_2_index = {\"<PAD>\":0,\"<UNK>\":1}\n",
    "    for text in train_texts:\n",
    "        for word in text:\n",
    "            word_2_index[word] = word_2_index.get(word,len(word_2_index))\n",
    "    return word_2_index,nn.Embedding(len(word_2_index),embedding_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self,all_text,all_label,word_2_index,max_len):\n",
    "        self.all_text = all_text\n",
    "        self.all_label = all_label\n",
    "        self.word_2_index = word_2_index\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        text = self.all_text[index][:self.max_len]\n",
    "        label = int(self.all_label[index])\n",
    "\n",
    "        text_idx = [self.word_2_index.get(i,1) for i in text]\n",
    "        text_idx = text_idx + [0] * (self.max_len - len(text_idx))\n",
    "\n",
    "        text_idx = torch.tensor(text_idx).unsqueeze(dim=0)\n",
    "\n",
    "        return text_idx,label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self,kernel_s,embeddin_num,max_len,hidden_num):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Conv1d(in_channels=embeddin_num,out_channels=hidden_num,kernel_size=kernel_s) #  200 * 50 * 20 (batch *  embedding_num * text_len  )\n",
    "        self.act = nn.ReLU()\n",
    "        self.mxp = nn.MaxPool1d(kernel_size=(max_len-kernel_s+1))\n",
    "\n",
    "    def forward(self,batch_emb): # 200 * 50 * 20 (batch_size * embedding_size * text_len )\n",
    "        c = self.cnn.forward(batch_emb)\n",
    "        a = self.act.forward(c)\n",
    "        a = a.squeeze(dim=-1)\n",
    "        m = self.mxp.forward(a)\n",
    "        m = m.squeeze(dim=-1)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模块组成模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNNModel(nn.Module):\n",
    "    def __init__(self,emb_matrix,max_len,class_num,hidden_num):\n",
    "        super().__init__()\n",
    "        self.emb_num = emb_matrix.weight.shape[1]\n",
    "\n",
    "        self.block1 = Block(2,self.emb_num,max_len,hidden_num)\n",
    "        self.block2 = Block(3,self.emb_num,max_len,hidden_num)\n",
    "        self.block3 = Block(4,self.emb_num,max_len,hidden_num)\n",
    "        self.block4 = Block(5, self.emb_num, max_len, hidden_num)\n",
    "\n",
    "        self.emb_matrix = emb_matrix\n",
    "\n",
    "        self.classfier = nn.Linear(hidden_num*4,class_num) # 全连接层，可以视为分类器\n",
    "        self.loss_fun = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self,batch_idx,batch_label=None):\n",
    "        batch_emb = self.emb_matrix(batch_idx) # 输入维数 200 * 50 * 20\n",
    "        batch_emb = batch_emb.permute(0, 2, 1)\n",
    "        b1_result = self.block1.forward(batch_emb)\n",
    "        b2_result = self.block2.forward(batch_emb)\n",
    "        b3_result = self.block3.forward(batch_emb)\n",
    "        b4_result = self.block4.forward(batch_emb)\n",
    "\n",
    "        feature = torch.cat([b1_result,b2_result,b3_result,b4_result],dim=1) # 1 * 6 : [ batch * (3 * 2)]\n",
    "        pre = self.classfier(feature) # 存疑 讲解为概率，权重，与下面的预测值间的区别不是很清楚\n",
    "        \n",
    "        # 如果有标签，那么就输出损失值；否则输出预测值\n",
    "        if batch_label is not None:\n",
    "            loss = self.loss_fun(pre,batch_label)\n",
    "            return loss\n",
    "        else:\n",
    "            return torch.argmax(pre,dim=-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoc 0\n",
      "loss:1.231\n",
      "acc = 63.68%\n",
      "\n",
      "epoc 1\n",
      "loss:0.994\n",
      "acc = 70.87%\n",
      "\n",
      "epoc 2\n",
      "loss:0.873\n",
      "acc = 73.52%\n",
      "\n",
      "epoc 3\n",
      "loss:0.811\n",
      "acc = 75.32%\n",
      "\n",
      "epoc 4\n",
      "loss:0.759\n",
      "acc = 76.60%\n",
      "\n",
      "epoc 5\n",
      "loss:0.724\n",
      "acc = 77.46%\n",
      "\n",
      "epoc 6\n",
      "loss:0.705\n",
      "acc = 78.09%\n",
      "\n",
      "epoc 7\n",
      "loss:0.685\n",
      "acc = 78.58%\n",
      "\n",
      "epoc 8\n",
      "loss:0.672\n",
      "acc = 78.93%\n",
      "\n",
      "epoc 9\n",
      "loss:0.648\n",
      "acc = 79.32%\n",
      "\n",
      "epoc 10\n",
      "loss:0.632\n",
      "acc = 79.52%\n",
      "\n",
      "epoc 11\n",
      "loss:0.618\n",
      "acc = 79.66%\n",
      "\n",
      "epoc 12\n",
      "loss:0.606\n",
      "acc = 79.78%\n",
      "\n",
      "epoc 13\n",
      "loss:0.589\n",
      "acc = 79.85%\n",
      "\n",
      "epoc 14\n",
      "loss:0.573\n",
      "acc = 79.77%\n",
      "\n",
      "epoc 15\n",
      "loss:0.559\n",
      "acc = 80.16%\n",
      "\n",
      "epoc 16\n",
      "loss:0.549\n",
      "acc = 80.25%\n",
      "\n",
      "epoc 17\n",
      "loss:0.532\n",
      "acc = 80.30%\n",
      "\n",
      "epoc 18\n",
      "loss:0.521\n",
      "acc = 80.41%\n",
      "\n",
      "epoc 19\n",
      "loss:0.509\n",
      "acc = 80.38%\n",
      "\n",
      "epoc 20\n",
      "loss:0.499\n",
      "acc = 80.49%\n",
      "\n",
      "epoc 21\n",
      "loss:0.490\n",
      "acc = 80.48%\n",
      "\n",
      "epoc 22\n",
      "loss:0.483\n",
      "acc = 80.57%\n",
      "\n",
      "epoc 23\n",
      "loss:0.470\n",
      "acc = 80.53%\n",
      "\n",
      "epoc 24\n",
      "loss:0.463\n",
      "acc = 80.56%\n",
      "\n",
      "epoc 25\n",
      "loss:0.454\n",
      "acc = 80.57%\n",
      "\n",
      "epoc 26\n",
      "loss:0.447\n",
      "acc = 80.50%\n",
      "\n",
      "epoc 27\n",
      "loss:0.439\n",
      "acc = 80.63%\n",
      "\n",
      "epoc 28\n",
      "loss:0.435\n",
      "acc = 80.49%\n",
      "\n",
      "epoc 29\n",
      "loss:0.428\n",
      "acc = 80.53%\n",
      "\n",
      "epoc 30\n",
      "loss:0.420\n",
      "acc = 80.55%\n",
      "\n",
      "epoc 31\n",
      "loss:0.411\n",
      "acc = 80.65%\n",
      "\n",
      "epoc 32\n",
      "loss:0.405\n",
      "acc = 80.63%\n",
      "\n",
      "epoc 33\n",
      "loss:0.398\n",
      "acc = 80.64%\n",
      "\n",
      "epoc 34\n",
      "loss:0.393\n",
      "acc = 80.63%\n",
      "\n",
      "epoc 35\n",
      "loss:0.389\n",
      "acc = 80.64%\n",
      "\n",
      "epoc 36\n",
      "loss:0.386\n",
      "acc = 80.66%\n",
      "\n",
      "epoc 37\n",
      "loss:0.380\n",
      "acc = 80.76%\n",
      "\n",
      "epoc 38\n",
      "loss:0.374\n",
      "acc = 80.65%\n",
      "\n",
      "epoc 39\n",
      "loss:0.369\n",
      "acc = 80.77%\n",
      "\n",
      "epoc 40\n",
      "loss:0.367\n",
      "acc = 80.77%\n",
      "\n",
      "epoc 41\n",
      "loss:0.360\n",
      "acc = 80.82%\n",
      "\n",
      "epoc 42\n",
      "loss:0.357\n",
      "acc = 80.73%\n",
      "\n",
      "epoc 43\n",
      "loss:0.353\n",
      "acc = 80.69%\n",
      "\n",
      "epoc 44\n",
      "loss:0.353\n",
      "acc = 80.62%\n",
      "\n",
      "epoc 45\n",
      "loss:0.349\n",
      "acc = 80.61%\n",
      "\n",
      "epoc 46\n",
      "loss:0.345\n",
      "acc = 80.61%\n",
      "\n",
      "epoc 47\n",
      "loss:0.343\n",
      "acc = 80.64%\n",
      "\n",
      "epoc 48\n",
      "loss:0.338\n",
      "acc = 80.61%\n",
      "\n",
      "epoc 49\n",
      "loss:0.336\n",
      "acc = 80.52%\n",
      "\n",
      "epoc 50\n",
      "loss:0.328\n",
      "acc = 80.55%\n",
      "\n",
      "epoc 51\n",
      "loss:0.326\n",
      "acc = 80.47%\n",
      "\n",
      "epoc 52\n",
      "loss:0.324\n",
      "acc = 80.49%\n",
      "\n",
      "epoc 53\n",
      "loss:0.321\n",
      "acc = 80.50%\n",
      "\n",
      "epoc 54\n",
      "loss:0.319\n",
      "acc = 80.48%\n",
      "\n",
      "epoc 55\n",
      "loss:0.316\n",
      "acc = 80.42%\n",
      "\n",
      "epoc 56\n",
      "loss:0.312\n",
      "acc = 80.58%\n",
      "\n",
      "epoc 57\n",
      "loss:0.312\n",
      "acc = 80.51%\n",
      "\n",
      "epoc 58\n",
      "loss:0.311\n",
      "acc = 80.45%\n",
      "\n",
      "epoc 59\n",
      "loss:0.308\n",
      "acc = 80.35%\n",
      "\n",
      "epoc 60\n",
      "loss:0.305\n",
      "acc = 80.32%\n",
      "\n",
      "epoc 61\n",
      "loss:0.304\n",
      "acc = 80.33%\n",
      "\n",
      "epoc 62\n",
      "loss:0.300\n",
      "acc = 80.30%\n",
      "\n",
      "epoc 63\n",
      "loss:0.299\n",
      "acc = 80.35%\n",
      "\n",
      "epoc 64\n",
      "loss:0.297\n",
      "acc = 80.34%\n",
      "\n",
      "epoc 65\n",
      "loss:0.294\n",
      "acc = 80.19%\n",
      "\n",
      "epoc 66\n",
      "loss:0.296\n",
      "acc = 80.15%\n",
      "\n",
      "epoc 67\n",
      "loss:0.295\n",
      "acc = 80.22%\n",
      "\n",
      "epoc 68\n",
      "loss:0.298\n",
      "acc = 80.11%\n",
      "\n",
      "epoc 69\n",
      "loss:0.296\n",
      "acc = 80.03%\n",
      "\n",
      "epoc 70\n",
      "loss:0.296\n",
      "acc = 80.03%\n",
      "\n",
      "epoc 71\n",
      "loss:0.293\n",
      "acc = 80.08%\n",
      "\n",
      "epoc 72\n",
      "loss:0.295\n",
      "acc = 80.16%\n",
      "\n",
      "epoc 73\n",
      "loss:0.292\n",
      "acc = 80.09%\n",
      "\n",
      "epoc 74\n",
      "loss:0.293\n",
      "acc = 80.07%\n",
      "\n",
      "epoc 75\n",
      "loss:0.292\n",
      "acc = 80.09%\n",
      "\n",
      "epoc 76\n",
      "loss:0.289\n",
      "acc = 80.08%\n",
      "\n",
      "epoc 77\n",
      "loss:0.288\n",
      "acc = 80.03%\n",
      "\n",
      "epoc 78\n",
      "loss:0.287\n",
      "acc = 80.00%\n",
      "\n",
      "epoc 79\n",
      "loss:0.288\n",
      "acc = 79.90%\n",
      "\n",
      "epoc 80\n",
      "loss:0.289\n",
      "acc = 79.95%\n",
      "\n",
      "epoc 81\n",
      "loss:0.286\n",
      "acc = 79.91%\n",
      "\n",
      "epoc 82\n",
      "loss:0.286\n",
      "acc = 79.82%\n",
      "\n",
      "epoc 83\n",
      "loss:0.282\n",
      "acc = 79.68%\n",
      "\n",
      "epoc 84\n",
      "loss:0.277\n",
      "acc = 79.74%\n",
      "\n",
      "epoc 85\n",
      "loss:0.278\n",
      "acc = 79.74%\n",
      "\n",
      "epoc 86\n",
      "loss:0.278\n",
      "acc = 79.66%\n",
      "\n",
      "epoc 87\n",
      "loss:0.275\n",
      "acc = 79.70%\n",
      "\n",
      "epoc 88\n",
      "loss:0.274\n",
      "acc = 79.66%\n",
      "\n",
      "epoc 89\n",
      "loss:0.274\n",
      "acc = 79.69%\n",
      "\n",
      "epoc 90\n",
      "loss:0.272\n",
      "acc = 79.66%\n",
      "\n",
      "epoc 91\n",
      "loss:0.271\n",
      "acc = 79.64%\n",
      "\n",
      "epoc 92\n",
      "loss:0.273\n",
      "acc = 79.58%\n",
      "\n",
      "epoc 93\n",
      "loss:0.267\n",
      "acc = 79.65%\n",
      "\n",
      "epoc 94\n",
      "loss:0.269\n",
      "acc = 79.69%\n",
      "\n",
      "epoc 95\n",
      "loss:0.267\n",
      "acc = 79.53%\n",
      "\n",
      "epoc 96\n",
      "loss:0.266\n",
      "acc = 79.58%\n",
      "\n",
      "epoc 97\n",
      "loss:0.263\n",
      "acc = 79.63%\n",
      "\n",
      "epoc 98\n",
      "loss:0.267\n",
      "acc = 79.49%\n",
      "\n",
      "epoc 99\n",
      "loss:0.262\n",
      "acc = 79.65%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_text,train_label = read_data(\"train\")\n",
    "validation_text,validation_label =  read_data(\"dev\")\n",
    "\n",
    "embedding = 50\n",
    "max_len= 20\n",
    "batch_size = 200\n",
    "epoch = 100\n",
    "lr = 0.001\n",
    "hidden_num = 2\n",
    "class_num = len(set(train_label))\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "word_2_index,words_embedding = built_corpus(train_text,embedding)\n",
    "\n",
    "train_dataset = TextDataset(train_text,train_label,word_2_index,max_len)\n",
    "train_loader = DataLoader(train_dataset,batch_size,shuffle=False)\n",
    "\n",
    "validation_dataset = TextDataset(validation_text,validation_label,word_2_index,max_len)\n",
    "validation_loader = DataLoader(validation_dataset,batch_size,shuffle=False)\n",
    "\n",
    "\n",
    "model = TextCNNModel(words_embedding,max_len,class_num,hidden_num).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(),lr=lr)\n",
    "\n",
    "\n",
    "for e in range(epoch):\n",
    "    print(f\"epoc {e}\")\n",
    "\n",
    "    for batch_idx,batch_label in train_loader:\n",
    "        batch_idx = torch.reshape(batch_idx,(batch_size,max_len))\n",
    "        batch_idx = batch_idx.to(device)\n",
    "        batch_label = batch_label.to(device)\n",
    "        loss = model.forward(batch_idx,batch_label)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    print(f\"loss:{loss:.3f}\")\n",
    "\n",
    "    right_num = 0\n",
    "    for batch_idx,batch_label in validation_loader:\n",
    "        batch_idx = torch.reshape(batch_idx,(batch_size,max_len))\n",
    "        batch_idx = batch_idx.to(device)\n",
    "        batch_label = batch_label.to(device)\n",
    "        pre = model.forward(batch_idx)\n",
    "        right_num += int(torch.sum(pre==batch_label))\n",
    "\n",
    "    print(f\"acc = {right_num/len(validation_text)*100:.2f}%\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb32b920de0eac9b9b0b3f8294f97bb0cdb08a238b6b4be4a063bc7e6f2866f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
