{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 导入Pytorch\n",
    "首先，我们导入`torch`。请注意，虽然它被称为PyTorch，但是代码中使用`torch`而不是`pytorch`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 数据操作\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 张量（Tensor）\n",
    "*张量*（Tensor）表示由一个数值组成的数组，这个数组可能有多个维度。\n",
    "具有一个维度的张量对应数学上的*向量*（vector）；\n",
    "具有两个维度的张量对应数学上的*矩阵*（matrix）；\n",
    "具有两个维度以上的张量没有特殊的数学名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2, -1,  0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = torch.tensor([1,2,-1,0])\n",
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  2.0000, -1.0000,  0.0000],\n",
       "        [ 1.2000,  3.0000,  0.5000,  0.5000]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = torch.tensor([[1,2,-1,0],[1.2,3,0.5,0.5]])\n",
    "mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "查看张量的形状和元素总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "改变张量的形状，注意这些函数返回一个新的张量而不是改变原有的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  2.0000],\n",
       "        [-1.0000,  0.0000],\n",
       "        [ 1.2000,  3.0000],\n",
       "        [ 0.5000,  0.5000]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.reshape((4,2)) #试试 mat.reshape((4,-1)) 的结果是什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  1.2000],\n",
       "        [ 2.0000,  3.0000],\n",
       "        [-1.0000,  0.5000],\n",
       "        [ 0.0000,  0.5000]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.T #转置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  2.0000, -1.0000,  0.0000,  1.2000,  3.0000,  0.5000,  0.5000]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.reshape((1,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0000,  2.0000],\n",
       "         [-1.0000,  0.0000]],\n",
       "\n",
       "        [[ 1.2000,  3.0000],\n",
       "         [ 0.5000,  0.5000]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.reshape((2,2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  2.0000],\n",
       "        [-1.0000,  0.0000],\n",
       "        [ 1.2000,  3.0000],\n",
       "        [ 0.5000,  0.5000]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.view((4,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "一些创建张量的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2917, 0.9144, 0.2233, 0.2825],\n",
       "        [0.0137, 0.5175, 0.7897, 0.5990],\n",
       "        [0.4418, 0.1101, 0.3607, 0.6593]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand((3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3,4], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor([1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 张量运算\n",
    "在张量上执行数学运算\n",
    "### 按元素计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.,  4.,  6., 10.]),\n",
       " tensor([-1.,  0.,  2.,  6.]),\n",
       " tensor([ 2.,  4.,  8., 16.]),\n",
       " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
       " tensor([ 1.,  4., 16., 64.]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2, 4, 8], dtype=torch.float)\n",
    "y = torch.tensor([2, 2, 2, 2], dtype=torch.float)\n",
    "x + y, x - y, x * y, x / y, x ** y  # **运算符是求幂运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "一元运算符，常见的激活函数都是按元素计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03]),\n",
       " tensor([0.7311, 0.8808, 0.9820, 0.9997]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(x),torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 线性代数运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "torch.mul()是矩阵的点乘，即对应的位相乘<br>\n",
    "torch.mm()是矩阵正常的矩阵相乘<br>\n",
    "torch.dot()类似于mul()，它是向量(即只能是一维的张量)的对应位相乘再求和，返回一个tensor数值<br>\n",
    "torch.mv()是矩阵和向量相乘，类似于torch.mm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(30.), tensor(30.))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(x,y), (x*y).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " torch.Size([5, 4]),\n",
       " torch.Size([4, 3]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "B = torch.ones(4, 3)\n",
    "\n",
    "A, B, A.size(), B.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 6.,  6.,  6.],\n",
       "         [22., 22., 22.],\n",
       "         [38., 38., 38.],\n",
       "         [54., 54., 54.],\n",
       "         [70., 70., 70.]]),\n",
       " torch.Size([5, 3]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(A,B), torch.mm(A,B).size() # 也可以用 torch.matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "矩阵向量乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 34.,  94., 154., 214., 274.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mv(A , x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "将矩阵向量乘法转化为矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 34.,  94., 154., 214., 274.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(A, x.reshape((-1,1))).reshape((-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 逻辑运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " tensor([[2., 1., 4., 3.],\n",
       "         [1., 2., 3., 4.],\n",
       "         [4., 3., 2., 1.]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "逻辑运算是按元素的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X == Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  True],\n",
       "        [ True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X > 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 组合张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " torch.Size([3, 4]),\n",
       " tensor([[2., 1., 4., 3.],\n",
       "         [1., 2., 3., 4.],\n",
       "         [4., 3., 2., 1.]]),\n",
       " torch.Size([3, 4]),\n",
       " tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 2.,  1.,  4.,  3.],\n",
       "         [ 1.,  2.,  3.,  4.],\n",
       "         [ 4.,  3.,  2.,  1.]]),\n",
       " torch.Size([6, 4]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]),\n",
       " torch.Size([3, 8]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, X.size(), Y, Y.size(), torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=0).size(), torch.cat((X, Y), dim=1), torch.cat((X, Y), dim=1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.]],\n",
       " \n",
       "         [[ 2.,  1.,  4.,  3.],\n",
       "          [ 1.,  2.,  3.,  4.],\n",
       "          [ 4.,  3.,  2.,  1.]]]),\n",
       " torch.Size([2, 3, 4]),\n",
       " tensor([[[ 0.,  1.,  2.,  3.],\n",
       "          [ 2.,  1.,  4.,  3.]],\n",
       " \n",
       "         [[ 4.,  5.,  6.,  7.],\n",
       "          [ 1.,  2.,  3.,  4.]],\n",
       " \n",
       "         [[ 8.,  9., 10., 11.],\n",
       "          [ 4.,  3.,  2.,  1.]]]),\n",
       " torch.Size([3, 2, 4]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack((X,Y), dim=0), torch.stack((X,Y), dim=0).size(), torch.stack((X,Y), dim=1), torch.stack((X,Y), dim=1).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 广播（broadcasting）\n",
    "在某些情况下，即使形状不同，我们仍然可以通过调用*广播机制*（broadcasting mechanism）来执行按元素操作。\n",
    "\n",
    "在大多数情况下，我们将沿着数组中长度为1的轴进行广播，如下例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1]]),\n",
       " torch.Size([3, 1]),\n",
       " torch.Size([1, 2]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "a, b, a.size(), b.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1],\n",
       "         [1, 2],\n",
       "         [2, 3]]),\n",
       " torch.Size([3, 2]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a+b,(a+b).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 索引和切片\n",
    "就像在任何其他Python数组中一样，张量中的元素可以通过索引访问。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " tensor([[2., 1., 4., 3.],\n",
       "         [1., 2., 3., 4.],\n",
       "         [4., 3., 2., 1.]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8.,  9., 10., 11.]),\n",
       " tensor([[ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " tensor([ 6., 10.]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-1], X[1:3], X[1:3,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "通过索引/切片赋值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0,0] = -1\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2., -2., -2., -2.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0,] = -2\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 将Tensor转化为其他Python对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, torch.Tensor)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = X.numpy() # Tensor to Numpy ndarray\n",
    "B = torch.tensor(A) # Numpy ndarray to Tensor\n",
    "type(A), type(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[-2.0, -2.0, -2.0, -2.0], [4.0, 5.0, 6.0, 7.0], [8.0, 9.0, 10.0, 11.0]],\n",
       " list)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = X.tolist()\n",
    "A, type(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "当Tensor是一个标量时，可以调用`item`函数或Python的内置函数直接得到标量值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "介绍在pytorch中定义神经网络的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 基础神经网络组件\n",
    "`torch.nn`模块中提供了常用的神经网络组件。包括线性层、CNN、RNN等，利用它们可以方便地快速构建深度神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "线性层定义了全连接神经网路中的线性计算部分（不包括激活函数）。\n",
    "我们将两个参数传递到`nn.Linear`中。 第一个指定输入特征形状，第二个指定输出特征形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "linear_net = nn.Linear(3, 4) #输入为3个神经元，输出为4个神经元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "linear_net = nn.Linear(3, 4, bias=False) #不带bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "查看参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 0.0229, -0.1887,  0.1731],\n",
       "         [-0.1718, -0.1275,  0.1270],\n",
       "         [-0.4737, -0.2492, -0.2789],\n",
       "         [ 0.0959, -0.2026,  0.1347]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.1119,  0.4424, -0.4808, -0.1469], requires_grad=True))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_net = nn.Linear(3, 4) #输入为3个神经元，输出为4个神经元\n",
    "linear_net.weight, linear_net.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "调用全连接层进行计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7580, 0.7440, 0.9484])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((3))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0707,  0.3377, -1.2898, -0.0973], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = linear_net(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "并行计算\n",
    "全连接层的线性变换计算在张量的最后一个维度，在其他维度并行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 4])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((5,4,3))\n",
    "y = linear_net(x)\n",
    "y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "其他基础模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2维卷积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 28, 28])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_net = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, padding=1) # 输入通道（特征）为3，输出通道6，卷积核大小3（3x3的方形）, 设置padding为1来构成等宽卷积.\n",
    "x = torch.rand((4, 3, 28, 28)) # 4张图片，每张图片大小为28x28，有RGB3个通道。\n",
    "y = conv_net(x)\n",
    "y.size() # 等宽卷积"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2维池化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 14, 14])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool_net = nn.MaxPool2d(2) # 池化核大小为2（2x2方形），可以将图片大小缩小一倍\n",
    "z = pool_net(y)\n",
    "z.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 8]), torch.Size([1, 2, 8]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_net = nn.RNN(input_size = 6, hidden_size = 8, num_layers=1, nonlinearity='relu', bidirectional=False, batch_first=True)\n",
    "x = torch.rand((2, 5, 6)) # 2个序列，每个序列长度为5，序列每个位置为一个6维向量\n",
    "y = rnn_net(x)\n",
    "y[0].size(), y[1].size() #输出为一个tuple，分别表示每个位置的hidden，和最后一个位置的hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1493, 0.1086, 0.8944, 0.0000, 0.2180, 0.0000, 0.0209, 0.0000],\n",
       "         [0.0000, 0.0000, 0.8800, 0.0000, 0.0821, 0.1595, 0.0000, 0.0000]],\n",
       "        grad_fn=<SelectBackward0>),\n",
       " tensor([[0.1493, 0.1086, 0.8944, 0.0000, 0.2180, 0.0000, 0.0209, 0.0000],\n",
       "         [0.0000, 0.0000, 0.8800, 0.0000, 0.0821, 0.1595, 0.0000, 0.0000]],\n",
       "        grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0][:,-1], y[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 通过基础组件构建自定义的网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "pytorch中所有的网络都继承自`nn.Module`，要定义一个新的网络，同样需要继承`nn.Module`，然后重写`forward`函数，在`forward`中定义计算过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "比如定义一个完整的全连接网络（线性层+激活函数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self, in_features, out_features) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x:Tensor):\n",
    "        return self.activation(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.4303, 0.2475, 0.0000], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MyNet(in_features=3, out_features=4)\n",
    "\n",
    "x = torch.rand((3,))\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "定义深度网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MyDeepNet(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_layer_num) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_layer_list = [MyNet(in_features=in_features, out_features=in_features) for _ in range(hidden_layer_num)]\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList(hidden_layer_list) # 注意 `self.hidden_layers = hidden_layer_list`是不行的，因为hidden_layer_list并不继承自`nn.Module`\n",
    "\n",
    "        self.out_layers = MyNet(in_features=in_features, out_features=out_features)\n",
    "\n",
    "    def forward(self, x:Tensor):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        return self.out_layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MyDeepNet(in_features=3, out_features=1, hidden_layer_num=3)\n",
    "\n",
    "x = torch.rand((3,))\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 使用Pytorch进行深度学习的主要流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "在实践中，我们讲深度学习应用在各种任务上的一般流程为：\n",
    "* 准备和处理数据\n",
    "* 划分数据集\n",
    "* 定义模型\n",
    "* 训练模型\n",
    "* 评估模型\n",
    "\n",
    "下面，我们讲以姓名分类任务为例，演示我们完成这些流程的实际代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 任务介绍\n",
    "输入一个由拉丁字母表示的名字，预测这个名字对应的语言。\n",
    "\n",
    "具体来说，我们将训练来自 18 种语言的几千个姓氏，并根据拼写预测一个名字来自哪种语言：\n",
    "> Christy -> English\n",
    ">\n",
    "> Lian -> Chinese\n",
    ">\n",
    "> Masuko -> Japanese\n",
    "\n",
    "参考资料：[NLP FROM SCRATCH: CLASSIFYING NAMES WITH A CHARACTER-LEVEL RNN](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 准备数据\n",
    "> Download the data from [here](https://download.pytorch.org/tutorial/data.zip) and extract it to the current directory.\n",
    "\n",
    "data/names 目录中包含 18 个名为“[Language].txt”的文本文件。每个文件包含多个人名，每行一个人名，大部分是拉丁字母（但我们仍然需要从 Unicode 转换为 ASCII）。\n",
    "\n",
    "我们最终会得到一个包含每种语言名字列表的字典，{language: [names ...]}。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/names\\\\Arabic.txt',\n",
       " 'data/names\\\\Chinese.txt',\n",
       " 'data/names\\\\Czech.txt',\n",
       " 'data/names\\\\Dutch.txt',\n",
       " 'data/names\\\\English.txt',\n",
       " 'data/names\\\\French.txt',\n",
       " 'data/names\\\\German.txt',\n",
       " 'data/names\\\\Greek.txt',\n",
       " 'data/names\\\\Irish.txt',\n",
       " 'data/names\\\\Italian.txt',\n",
       " 'data/names\\\\Japanese.txt',\n",
       " 'data/names\\\\Korean.txt',\n",
       " 'data/names\\\\Polish.txt',\n",
       " 'data/names\\\\Portuguese.txt',\n",
       " 'data/names\\\\Russian.txt',\n",
       " 'data/names\\\\Scottish.txt',\n",
       " 'data/names\\\\Spanish.txt',\n",
       " 'data/names\\\\Vietnamese.txt']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "findFiles('data/names/*.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "定义unicode转Ascii的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Slusarski'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "unicodeToAscii('Ślusàrski')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "处理原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ang', 'AuYong', 'Bai', 'Ban', 'Bao']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_lines['Chinese'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "分词并处理为数据对$(x,y)$的样式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S', 'l', 'u', 's', 'a', 'r', 's', 'k', 'i']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_line(name:str):\n",
    "    return list(name)\n",
    "\n",
    "split_line(\"Slusarski\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['K', 'h', 'o', 'u', 'r', 'y'], 'Arabic'),\n",
       " (['N', 'a', 'h', 'a', 's'], 'Arabic'),\n",
       " (['D', 'a', 'h', 'e', 'r'], 'Arabic'),\n",
       " (['G', 'e', 'r', 'g', 'e', 's'], 'Arabic'),\n",
       " (['N', 'a', 'z', 'a', 'r', 'i'], 'Arabic')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pairs = []\n",
    "for category_name, lines in category_lines.items():\n",
    "    for name in lines:\n",
    "        data_pairs.append((split_line(name), category_name))\n",
    "\n",
    "data_pairs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20074"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "划分训练集、验证集和测试集。\n",
    "\n",
    "我们随机选取2000个样本作为验证集、2000个样本作为测试集，剩下的作为训练集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['B', 'e', 'r', 'g', 'e', 'r'], 'French'),\n",
       " (['L', 'e', 'm', 'm', 'i'], 'Italian'),\n",
       " (['O', 'c', 'a', 's', 'k', 'o', 'v', 'a'], 'Czech'),\n",
       " (['I', 'w', 'a', 's', 'a'], 'Japanese'),\n",
       " (['F', 'i', 'n', 'n', 'i', 'g', 'a', 'n'], 'English')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(114514)\n",
    "\n",
    "index = list(range(len(data_pairs)))\n",
    "\n",
    "random.shuffle(index)\n",
    "\n",
    "valid_index = index[:2000]\n",
    "test_index = index[2000:4000]\n",
    "train_index = index[4000:]\n",
    "\n",
    "valid_pairs = [data_pairs[i] for i in valid_index]\n",
    "test_pairs = [data_pairs[i] for i in test_index]\n",
    "train_pairs = [data_pairs[i] for i in train_index]\n",
    "\n",
    "valid_pairs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "构建Pytorch Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "首先定义一个字典类，方便我们将字符串编码为一个数字ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class WordDict:\n",
    "    def __init__(self) -> None:\n",
    "        self.index2word = {}\n",
    "        self.word2index = {}\n",
    "        self.dict_size = 0\n",
    "\n",
    "    \n",
    "    def add_word(self, word:str):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.dict_size\n",
    "            self.index2word[self.dict_size] = word\n",
    "            self.dict_size += 1\n",
    "    \n",
    "    def index(self, word:str):\n",
    "        if word in self.word2index:\n",
    "            return self.word2index[word]\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    def word(self, index:int):\n",
    "        if index in self.index2word:\n",
    "            return self.index2word[index]\n",
    "        else:\n",
    "            return \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arabic': 0,\n",
       " 'Chinese': 1,\n",
       " 'Czech': 2,\n",
       " 'Dutch': 3,\n",
       " 'English': 4,\n",
       " 'French': 5,\n",
       " 'German': 6,\n",
       " 'Greek': 7,\n",
       " 'Irish': 8,\n",
       " 'Italian': 9,\n",
       " 'Japanese': 10,\n",
       " 'Korean': 11,\n",
       " 'Polish': 12,\n",
       " 'Portuguese': 13,\n",
       " 'Russian': 14,\n",
       " 'Scottish': 15,\n",
       " 'Spanish': 16,\n",
       " 'Vietnamese': 17}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_dict = WordDict()\n",
    "category_dict = WordDict()\n",
    "\n",
    "name_dict.add_word(\"<pad>\")\n",
    "for name, category_name in data_pairs:\n",
    "    for char in name:\n",
    "        name_dict.add_word(char)\n",
    "    category_dict.add_word(category_name)\n",
    "\n",
    "category_dict.word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 8)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_dict.index('A'), name_dict.index('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "构建Dataset，我们的Dataset需要继承自`torch.utils.data.Dataset`，并重写`__get_item__`方法和`__len__`方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import Tuple, List\n",
    "\n",
    "class NameDataset(Dataset):\n",
    "    def __init__(self, pairs:Tuple[List[str], str], name_dict:WordDict, category_dict:WordDict) -> None:\n",
    "        super().__init__()\n",
    "        self.pairs = pairs\n",
    "        self.name_dict = name_dict\n",
    "        self.category_dict = category_dict\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        name, category = self.pairs[index]\n",
    "        x = torch.LongTensor([self.name_dict.index(char) for char in name])\n",
    "        y = torch.LongTensor([self.category_dict.index(category)])\n",
    "        return (x,y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([45,  5, 15, 27, 49,  3,  5,  3, 49]), tensor([14]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset = NameDataset(valid_pairs, name_dict, category_dict)\n",
    "test_dataset = NameDataset(test_pairs, name_dict, category_dict)\n",
    "train_dataset = NameDataset(train_pairs, name_dict, category_dict)\n",
    "\n",
    "valid_dataset[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "初始化DataLoader，通过DataLoader自动地从数据集中抽取Batch。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "在本任务中，由于输入的长度不是固定的，同一个Batch中输入的长度各不相同，我们使用<pad> token对输入进行填充。\n",
    "\n",
    "比如输入为\n",
    "```\n",
    "19, 11,  5, 13, 11,  5\n",
    "43, 11, 32, 32, 15\n",
    "44, 39,  8,  9, 33,  3, 49,  8\n",
    "30, 28,  8,  9,  8\n",
    "```\n",
    "填充到Batch中最长的长度\n",
    "```\n",
    "19, 11,  5, 13, 11,  5,  0,  0\n",
    "43, 11, 32, 32, 15,  0,  0,  0\n",
    "44, 39,  8,  9, 33,  3, 49,  8\n",
    "30, 28,  8,  9,  8,  0,  0,  0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "可以在`collate_fn`函数中做这些处理，然后DataLoader会自动调用这个函数对Batch进行处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(pair_list):\n",
    "    input_list, label_list = zip(*pair_list)\n",
    "    \n",
    "    max_len = max([len(input_tensor) for input_tensor in input_list])\n",
    "\n",
    "    collated_input = []\n",
    "    for input_tensor in input_list:\n",
    "        padding_len = max_len - len(input_tensor)\n",
    "        padding_tensor = torch.zeros((padding_len,), dtype=torch.long)\n",
    "        padding_tensor[:] = name_dict.index(\"<pad>\")\n",
    "        collated_input.append(torch.cat([input_tensor, padding_tensor], dim=0))\n",
    "    \n",
    "    collated_input = torch.stack(collated_input, dim=0)\n",
    "    collated_label = torch.cat(label_list, dim=0)\n",
    "    return collated_input, collated_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "打印DataLoader第一个Batch中前4行，可以看到padding后的输入，以及对应的label。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19, 11,  5, 13, 11,  5,  0,  0,  0,  0,  0],\n",
      "        [43, 11, 32, 32, 15,  0,  0,  0,  0,  0,  0],\n",
      "        [44, 39,  8,  9, 33,  3, 49,  8,  0,  0,  0],\n",
      "        [30, 28,  8,  9,  8,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([ 5,  9,  2, 10])\n"
     ]
    }
   ],
   "source": [
    "for batch in valid_dataloader:\n",
    "    print(batch[0][:4])\n",
    "    print(batch[1][:4])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 定义神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "作为一个baseline，我们使用一个3层卷积网络进行分类\n",
    "\n",
    "![网络架构](./diagram/NameCNN.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NameCNN(nn.Module):\n",
    "    def __init__(self, embedding_size, embedding_dim, out_features, dropout) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(embedding_size, embedding_dim)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(embedding_dim, 2*embedding_dim, 3, padding=1)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(2*embedding_dim, 4*embedding_dim, 3, padding=1)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(4*embedding_dim, 8*embedding_dim, 3, padding=1)\n",
    "\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "\n",
    "        self.linear = nn.Linear(8*embedding_dim, out_features)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x:Tensor):\n",
    "        x = self.embedding(x) # B x T -> B x T x C\n",
    "\n",
    "        x = x.transpose(1,2) # B x T x C -> B x C x T\n",
    "\n",
    "        x = self.conv1(x) # B x C x T -> B x 2C x T\n",
    "\n",
    "        x = nn.functional.relu(x) # relu本身不带参数，使用nn.functional.relu不需要初始化一个relu层组件\n",
    "\n",
    "        x = self.pool1(x) # B x 2C x T -> B x 2C x T/2\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv2(x) # B x 2C x T/2 -> B x 4C x T/2\n",
    "\n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        x = self.pool2(x) # B x 4C x T/2 -> B x 4C x T/4\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv3(x) # B x 4C x T/4 -> B x 8C x T/4\n",
    "\n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        x = x.max(dim=2)[0] # final max pool  B x 8C x T/4 -> B x 8C\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.linear(x) # B x 8C -> B x class_num\n",
    "\n",
    "        return x # 这里没有用softmax转化为概率，我们在loss function中完成这一步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "首先需要初始化训练模型的各个组件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "初始化dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dataloader\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn) # shuffle=True打乱采样顺序（随机抽取Batch）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = NameCNN(embedding_size=name_dict.dict_size, embedding_dim=64, out_features=category_dict.dict_size, dropout=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "初始化optimizer\n",
    "\n",
    "optimizer用于执行梯度更新：\n",
    "\n",
    "$\\theta_{t+1}=\\theta_t -\\alpha \\frac{\\partial \\mathcal{L}_{\\mathcal{D}}(\\theta)}{\\partial \\theta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=5e-4, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "初始化loss function\n",
    "\n",
    "由于是分类任务，我们使用交叉熵损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(reduction = \"sum\") # 先把每个Batch的loss加起来不平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model:nn.Module, optimizer:torch.optim.Optimizer, loss_func, dataloader):\n",
    "    sum_loss = 0\n",
    "    train_num = 0\n",
    "    \n",
    "    model.train() # 将model设置为训练状态\n",
    "    for input_data, label in dataloader:\n",
    "        \n",
    "        optimizer.zero_grad() # 初始化梯度\n",
    "\n",
    "        pred = model(input_data)\n",
    "\n",
    "        loss:Tensor = loss_func(pred, label)\n",
    "\n",
    "        sum_loss += loss.detach()\n",
    "        train_num += len(input_data)\n",
    "\n",
    "        # 对loss求导数\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step() # 更新参数\n",
    "    \n",
    "    print(f\"Avg loss:{(sum_loss/train_num).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model:nn.Module, dataloader):\n",
    "    acc_num = 0\n",
    "    all_num = 0\n",
    "\n",
    "    model.eval() # 将model设置为评估状态\n",
    "    for input_data, label in dataloader:\n",
    "        pred = model(input_data)\n",
    "        acc_num += (pred.max(dim=1)[1] == label).sum().item()\n",
    "        all_num += len(input_data)\n",
    "    \n",
    "    print(f\"acc rate: {acc_num/all_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "训练\n",
    "\n",
    "该模型可以在普通PC的CPU上完成训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss:1.2522339820861816\n",
      "acc rate: 0.751\n",
      "Avg loss:0.8740172386169434\n",
      "acc rate: 0.788\n",
      "Avg loss:0.7239394783973694\n",
      "acc rate: 0.796\n",
      "Avg loss:0.6569985747337341\n",
      "acc rate: 0.8055\n",
      "Avg loss:0.5928605794906616\n",
      "acc rate: 0.818\n",
      "Avg loss:0.5466974377632141\n",
      "acc rate: 0.8195\n",
      "Avg loss:0.5088253617286682\n",
      "acc rate: 0.8205\n",
      "Avg loss:0.483119398355484\n",
      "acc rate: 0.816\n",
      "Avg loss:0.45153918862342834\n",
      "acc rate: 0.82\n",
      "Avg loss:0.42957034707069397\n",
      "acc rate: 0.8245\n",
      "Avg loss:0.4154905080795288\n",
      "acc rate: 0.825\n",
      "Avg loss:0.38534820079803467\n",
      "acc rate: 0.838\n",
      "Avg loss:0.3716210722923279\n",
      "acc rate: 0.8265\n",
      "Avg loss:0.3594037592411041\n",
      "acc rate: 0.819\n",
      "Avg loss:0.34742796421051025\n",
      "acc rate: 0.818\n",
      "Avg loss:0.3294607698917389\n",
      "acc rate: 0.8295\n",
      "Avg loss:0.32456493377685547\n",
      "acc rate: 0.838\n",
      "Avg loss:0.3059841990470886\n",
      "acc rate: 0.827\n",
      "Avg loss:0.30217546224594116\n",
      "acc rate: 0.8215\n",
      "Avg loss:0.28811362385749817\n",
      "acc rate: 0.823\n"
     ]
    }
   ],
   "source": [
    "epoch_n = 20\n",
    "\n",
    "for i in range(epoch_n):\n",
    "    train(model, optimizer, loss_func, train_dataloader)\n",
    "    evaluate(model, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc rate: 0.8275\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 课后实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我们的课后实践部分仍然基于姓名分类任务，目标是设计并实现一个基于RNN的模型完成姓名分类的任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 数据集\n",
    "为了方便在各个模型间进行比较，我们给出一个已经划分好训练数据和测试数据的数据集，而不使用随机划分的训练/验证/测试集。\n",
    "\n",
    "下载并解压数据集：[tutorial_names.tar.gz](https://dl.fbaipublicfiles.com/fairseq/data/tutorial_names.tar.gz)\n",
    "\n",
    "数据集已经经过Unicode转ASCII处理，并将输入拆分为字符形式。\n",
    "\n",
    "你仍然需要将输入和输出通过字典映射到一个整型数字上，方便转换为Tensor。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 模型\n",
    "\n",
    "1. 你首先需要在给出数据上重新训练我们给出NameCNN模型，并评估其在测试集上的表现作为baseline。\n",
    "2. 你需要设计实现一个基于RNN的分类模型，然后完成训练和评估，并与baseline进行比较。你的模型至少应该达到NameCNN相当的性能。\n",
    "\n",
    "tips：\n",
    "1. 可以使用基础的RNN单元，也可以尝试GRU、LSTM等拓展的RNN模型，这些都在Pytorch中有提供。\n",
    "2. 可以尝试使用多层RNN构成深度网络，尝试使用单向和双向的RNN。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 训练\n",
    "tips：\n",
    "1. 尝试条件一些超参数（比如训练轮数epoch_n，学习率lr），能否得到更好的结果。\n",
    "2. 可以使用[其他优化器](https://pytorch.org/docs/stable/optim.html#algorithms)。\n",
    "3. 可以使用[自适应学习率](https://pytorch.org/docs/stable/optim.html#algorithms)。\n",
    "\n",
    "### 并行化训练需要注意的问题\n",
    "注意到之前我们为了同时训练一个Batch的数据，对不同长度的序列添加\\<pad\\> token来统一长度，这可能对模型的预测产生负面影响。特别是在RNN中需要额外注意这个问题，你可以使用[`torch.nn.utils.rnn.pack_sequence`](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence)和[`torch.nn.utils.rnn.pack_padded_sequence`](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_sequence.html#torch-nn-utils-rnn-pack-sequence)，然后Pytorch可以自动地处理长度问题。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "eb32b920de0eac9b9b0b3f8294f97bb0cdb08a238b6b4be4a063bc7e6f2866f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
